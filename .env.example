# =============================================================================
# OpenFactory Environment Configuration
# =============================================================================
# Copy this file to .env and fill in the required values.
# Required values are marked with (required). Optional values have defaults.
#
# SECURITY: Never commit .env files. The .gitignore already excludes them.
# =============================================================================

# -----------------------------------------------------------------------------
# Database (PostgreSQL + pgvector)
# -----------------------------------------------------------------------------
POSTGRES_USER=openfactory
POSTGRES_PASSWORD=                    # (required) Database password
POSTGRES_DB=openfactory
POSTGRES_PORT=5432

# Full connection URL (constructed automatically in docker-compose)
# Set this directly when running outside Docker:
DATABASE_URL=postgresql://openfactory:password@localhost:5432/openfactory

# -----------------------------------------------------------------------------
# Redis (BullMQ jobs, Hocuspocus pub/sub, session cache)
# -----------------------------------------------------------------------------
REDIS_PASSWORD=                       # (required) Redis password
REDIS_PORT=6379
REDIS_MAXMEMORY=256mb

# Full connection URL (constructed automatically in docker-compose)
# Set this directly when running outside Docker:
REDIS_URL=redis://:password@localhost:6379

# -----------------------------------------------------------------------------
# Authentication (Better Auth)
# -----------------------------------------------------------------------------
BETTER_AUTH_SECRET=                    # (required) Auth encryption secret (min 32 chars)
BETTER_AUTH_URL=http://localhost:3000  # Public URL of the application

# GitHub OAuth (optional, for GitHub login)
GITHUB_CLIENT_ID=
GITHUB_CLIENT_SECRET=

# Google OAuth (optional, for Google login)
GOOGLE_CLIENT_ID=
GOOGLE_CLIENT_SECRET=

# -----------------------------------------------------------------------------
# API Server
# -----------------------------------------------------------------------------
NODE_ENV=production
API_PORT=3001
API_HOST=0.0.0.0
CORS_ORIGIN=                          # Allowed CORS origin (defaults to '*' in dev)
LOG_LEVEL=info                        # pino log level: trace | debug | info | warn | error

# -----------------------------------------------------------------------------
# Web Application (Next.js)
# -----------------------------------------------------------------------------
WEB_PORT=3000
NEXT_PUBLIC_API_URL=/api              # API base URL (relative when behind Nginx)
NEXT_PUBLIC_WS_URL=                   # WebSocket URL for real-time updates
NEXT_PUBLIC_HOCUSPOCUS_URL=           # Hocuspocus URL for collaborative editing

# -----------------------------------------------------------------------------
# Hocuspocus (Collaborative Editing Server)
# -----------------------------------------------------------------------------
HOCUSPOCUS_PORT=3002

# -----------------------------------------------------------------------------
# Nginx Reverse Proxy
# -----------------------------------------------------------------------------
NGINX_PORT=80
NGINX_SSL_PORT=443

# -----------------------------------------------------------------------------
# LLM Providers (at least one required for AI features)
# -----------------------------------------------------------------------------

# OpenAI
OPENAI_API_KEY=

# Anthropic
ANTHROPIC_API_KEY=

# Ollama (self-hosted, for local models)
OLLAMA_BASE_URL=                      # e.g., http://host.docker.internal:11434

# LLM Provider and Model Configuration
LLM_PROVIDER=                         # (required) openai | anthropic | ollama
LLM_MODEL=                            # e.g., gpt-4o, claude-sonnet-4-20250514, llama3
LLM_TEMPERATURE=0.7                   # Model temperature (0.0 - 1.0)
LLM_MAX_TOKENS=4096                   # Max tokens per response

# Embedding configuration
EMBEDDING_PROVIDER=                   # openai | ollama (defaults to LLM_PROVIDER)
EMBEDDING_MODEL=                      # e.g., text-embedding-3-small

# Legacy alias (deprecated, use LLM_MODEL instead)
DEFAULT_LLM_MODEL=

# -----------------------------------------------------------------------------
# GitHub App (optional, for codebase integration)
# -----------------------------------------------------------------------------
GITHUB_APP_ID=
GITHUB_APP_PRIVATE_KEY=               # Base64-encoded private key
GITHUB_APP_WEBHOOK_SECRET=            # Webhook signature verification secret

# -----------------------------------------------------------------------------
# S3-Compatible Object Storage (for artifacts)
# -----------------------------------------------------------------------------
# Use MinIO for self-hosted, or AWS S3 / any S3-compatible service
S3_ENDPOINT=http://localhost:9000     # e.g., http://minio:9000 or https://s3.amazonaws.com
S3_BUCKET=openfactory
S3_ACCESS_KEY=
S3_SECRET_KEY=
S3_REGION=us-east-1

# -----------------------------------------------------------------------------
# Slack Integration (optional, for notifications)
# -----------------------------------------------------------------------------
SLACK_WEBHOOK_URL=                    # Incoming webhook URL for alerts

# -----------------------------------------------------------------------------
# Jira Integration (optional, for issue creation)
# -----------------------------------------------------------------------------
JIRA_BASE_URL=                        # e.g., https://your-org.atlassian.net
JIRA_EMAIL=                           # Service account email
JIRA_API_TOKEN=                       # API token for the service account
JIRA_PROJECT_KEY=                     # Default project key
